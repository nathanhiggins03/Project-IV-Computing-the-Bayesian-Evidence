#Need to change to incorporate more efficient code for HME
library(ggplot2)

Sim <- 30
MC_values <- c(10,25, 50,500)   # <-- choose MC sizes

df_all <- data.frame()


for (MC_sample in MC_values) {
  
  time_simulation <- rep(0, Sim)
  est_simulation  <- rep(0, Sim)
  
  for (k in 1:Sim) {
    
    start=proc.time()
    
    #Bayesian Linear Regression with unknown beta and precision
    
    #Set seed for reproducibility
    set.seed(123 + k)
    #Data
    #y=2+3x+ϵ,ϵ∼N(0,0.5^2)
    
    N <- 30
    x <- runif(N, 0, 5)
    X <- cbind(1, x)   # include intercept
    beta_true <- c(2, 3)
    sigma_true <- 0.5
    y <- as.vector(X %*% beta_true + rnorm(N, 0, sigma_true))
    
    #Data we have is
    X
    y
    
    #Prior inputs
    d <- ncol(X)
    m0 <- rep(1, d)
    Lambda0 <- diag(5, d)   # vague prior
    alpha0 <- 3
    beta0  <- 36
    
    
    #Posterior distribution inputs
    LambdaN <- Lambda0 + t(X) %*% X
    mN <- solve(LambdaN, Lambda0 %*% m0 + t(X) %*% y)
    alphaN <- alpha0 + N / 2
    betaN <- beta0 + 0.5 * (t(y) %*% y + t(m0) %*% Lambda0 %*% m0 - t(mN) %*% LambdaN %*% mN)
    
    
    #Chibs Method
    
    #First need to set up a gibbs sampler for the model. Need FCDs found by getting joint posterior up to 
    #proportionality and getting rid of any terms that don't have beta/sigma_sq for FCD of beta/sigma_sq respectively
    library(mvtnorm)
    library(extraDistr)
    
    gibbs <- function(N,d, X,y, m0, Lambda0, alpha0, beta0) {
      #Initialise with prior means
      mat <- matrix(0, ncol=3, nrow=N)
      Beta <- m0
      sigma_sq <- beta0 / (alpha0-1)
      mat[1,]<-c(Beta,sigma_sq)
      
      #FCD paramters
      n <- length(y)
      for (i in 2:N) {
        LambdaN<- Lambda0 + t(X)%*%X
        mN <- solve(LambdaN, Lambda0 %*% m0 + t(X) %*% y)
        Beta <- as.numeric(rmvnorm(1, mean = mN, sigma = sigma_sq * solve(LambdaN)))
        
        alphanN<- alpha0 + (n+d)/2
        epsilon <- y - X %*% Beta
        betaN<- beta0 + 0.5 * (t(epsilon) %*% epsilon + t(Beta - m0) %*% Lambda0 %*% (Beta - m0))
        sigma_sq <- rinvgamma(1, alpha = alphanN, beta = betaN) 
        mat[i,] <- c(Beta, sigma_sq)
      }
      return(mat)
    }
    
    set.seed(3421)
    out1=gibbs(N=MC_sample,d=d, X=X,y=y, m0=m0, Lambda0=Lambda0, alpha0=alpha0, beta0=beta0)
    #Column mean
    colMeans(out1)
    #Compare to STAN posterior samples
    #print(posterior_sample)
    
    #l(theta)
    l_theta<- function(theta, X, y){
      Beta<- theta[1:2]
      sigma_sq<-theta[3]
      mean<-X %*% Beta
      sd <- sqrt(sigma_sq)
      out <- sum(dnorm(y, mean = mean, sd = sd, log = TRUE)) +
        dmvnorm(Beta, mean = m0, sigma = sigma_sq * solve(Lambda0), log = TRUE) +
        dinvgamma(sigma_sq, alpha0, beta0, log = TRUE)
      return(out)
    }
    
    
    #Hessian using optim function- more general as only need l_theta function
    optimiser<- optim(par= c(2,3,0.5), fn=l_theta,X=X, y=y,hessian = TRUE,control = list(fnscale = -1))
    #Note we can extract optimal theta(mode) from this
    theta_mode<-optimiser$par
    
    #Need to continue from here
    
    
    
    
    
    
    
    
    
    #Now estimate log posterior at theta*
    beta_gibbs<-out1[,1:2]
    sigma_sq_gibbs<- out1[,3]
    
    #We have access to this FCD(for Beta)
    LambdaN<- Lambda0 + t(X)%*%X
    mN <- solve(LambdaN, Lambda0 %*% m0 + t(X) %*% y)
    log_posterior_p1<-dmvnorm(theta_mode[1:2], mean = mN, sigma = theta_mode[3] * solve(LambdaN), log = TRUE)
    
    #Need to estimate this distribution using Monte Carlo using samples from FCD(gibbs samples)
    counter<-rep(0,length(sigma_sq_gibbs))
    for(i in 1:length(sigma_sq_gibbs)){
      n <- length(y)
      alphanN<- alpha0 + (n+d)/2
      epsilon <- y - X %*% beta_gibbs[i,]
      betaN<- beta0 + 0.5 * (t(epsilon) %*% epsilon + t(beta_gibbs[i,] - m0) %*% Lambda0 %*% (beta_gibbs[i,] - m0))
      counter[i] <- dinvgamma(theta_mode[3], alpha = alphanN, beta = betaN, log = TRUE) 
    }
    # log-sum-exp trick
    max_log <- max(counter)
    log_posterior_p2 <- max_log + log(mean(exp(counter - max_log)))
    
    #Complete log posterior
    log_posterior<- log_posterior_p1 + log_posterior_p2
    
    
    #Chibs estimator
    
    lprior<-dmvnorm(theta_mode[1:2], mean = m0, sigma = theta_mode[3] * solve(Lambda0), log = TRUE) + dinvgamma(theta_mode[3], alpha0, beta0, log = TRUE)
    llike<- sum(dnorm(y, mean = X %*% theta_mode[1:2], sd = sqrt(theta_mode[3]), log = TRUE))
    
    
    chibs<- lprior + llike - log_posterior
    
    est_simulation[k]<-chibs
    
    time_simulation[k] <- (proc.time() - start)[3]
  }
  
  #Store results
  df_all <- rbind(
    df_all,
    data.frame(
      Estimate = est_simulation,
      Time     = time_simulation,   # ← ADD THIS LINE
      MC = factor(paste0("N = ", MC_sample),
                  levels = paste0("N = ", MC_values))
    )
  )
  
}


#Plotting evidence violin plots against MC sample size
ggplot(df_all, aes(x = MC, y = Estimate)) +
  geom_violin(aes(fill = MC),
              trim = FALSE,
              alpha = 0.7) +
  geom_boxplot(width = 0.12,
               fill = "white",
               outlier.shape = NA) +
  geom_hline(aes(yintercept = true_le, color = "Analytical\nvalue"),
             linewidth = 1.2,
             na.rm = TRUE) +
  scale_color_manual(
    name = "",
    values = c("Analytical\nvalue" = "red")
  ) +
  labs(
    title = "Monte Carlo convergence of Chib's estimator",
    x = "N",
    y = "Log Evidence"
  ) +
  theme_minimal()+
  theme(
    legend.position = "right",
    legend.title = element_blank()
  )


#Mean runtime vs MC sample size
library(dplyr)

df_time <- df_all %>%
  group_by(MC) %>%
  summarise(
    mean_time = mean(Time),
    sd_time   = sd(Time),
    .groups = "drop"
  )

#ggplot(df_time, aes(x = MC, y = mean_time, group = 1)) +
#  geom_point(size = 3) +
#  geom_line() +
#  labs(
#    title = "Mean Runtime vs Monte Carlo Sample Size",
#    x = "Monte Carlo sample size",
#    y = "Mean runtime (seconds)"
#  ) +
#  theme_minimal()



#Plotting MC standard deviation against run time
df_error <- df_all %>%
  group_by(MC) %>%
  summarise(
    mc_sd = sd(Estimate),
    mc_bias = mean(Estimate) - true_le
  )
df_efficiency <- left_join(df_time, df_error, by = "MC")
ggplot(df_efficiency, aes(x = mean_time, y = mc_sd)) +
  geom_point(size = 3) +
  geom_line()+
  geom_text(aes(label = MC), vjust = -0.4, hjust=-0.2) +
  labs(
    title = "Monte Carlo efficiency - Chib's estimator",
    x = "Mean runtime (seconds)",
    y = "Monte Carlo SD"
  ) +
  coord_cartesian(xlim = c(NA, 0.125))+
  theme_minimal()
