#Need to change to incorporate more efficient code for HME
library(ggplot2)

Sim <- 30
MC_values <- c(100000)    # <-- choose MC sizes

df_all <- data.frame()

library(rstan)

rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

setwd("~/Desktop/Project IV")

posterior_model <- stan_model(
  file = "Posterior Mixture model.stan"
)


for (MC_sample in MC_values) {
  
  time_simulation <- rep(0, Sim)
  est_simulation  <- rep(0, Sim)
  
  for (k in 1:Sim) {
    
    start=proc.time()
    
    #Bayesian Linear Regression with unknown beta and precision
    
    #Set seed for reproducibility
    set.seed(123+k)
    
    #Data
    data(galaxies, package = "MASS")
    K <- 3   # or any K you want to test
    #Note paper divides units by 1000
    y<- galaxies/1000
    
    
    
    #Prior inputs
    N = length(y)
    K = K
    y = y
    
    alpha = rep(1, K)        # uniform Dirichlet
    mu0 = mean(y)     # data-centered prior
    lambda0 = 2.6/(max(y)-min(y))           # weak prior on means
    a0 = 1.28                   # weak Inv-Gamma
    b0 = 0.36*(mean(y^2) - (mean(y)^2))
    
    #(Log) Harmonic mean estimator
    
    
    #Sample from posterior using STAN
    #Stan Data
    stan_data<- list(
      N = N,
      K = K,
      y = y,
      
      alpha = alpha,        # uniform Dirichlet
      mu0 = mu0,     # data-centered prior
      lambda0 = lambda0,         # weak prior on means
      a0 = a0,                # weak Inv-Gamma
      b0 = b0
    )
    
    posterior_sample <- sampling(
      posterior_model,
      data = stan_data,
      iter = 2*MC_sample,
      chains = 1,
      refresh = 0
    )
    
    #Diagnostic checks
    #print(posterior_sample)
    #output = as.array(posterior_sample)
    #diagnostics(output)
    
    #Extract Prior samples
    post_sample_sigma_sq<- extract(posterior_sample, pars = 'sigma2')$'sigma2'  
    post_sample_omega<- extract(posterior_sample, pars = 'omega')$'omega'
    post_sample_mu<- extract(posterior_sample, pars = 'mu')$'mu'
    
    
    #Calculate log likelihood at each prior sample (mu,tau)
    
    S <- dim(post_sample_mu)[1]
    N <- length(y)
    K <- ncol(post_sample_mu)
    
    likelihood_log <- rep(0, S)
    
    for (s in 1:S) {
      
      likelihood_log[s] <- sum(
        sapply(y, function(yi) {
          
          # mixture log density at yi
          m <- max(
            log(post_sample_omega[s, ]) +
              dnorm(
                yi,
                mean = post_sample_mu[s, ],
                sd   = sqrt(post_sample_sigma_sq[s, ]),
                log  = TRUE
              )
          )
          
          m + log(sum(exp(
            log(post_sample_omega[s, ]) +
              dnorm(
                yi,
                mean = post_sample_mu[s, ],
                sd   = sqrt(post_sample_sigma_sq[s, ]),
                log  = TRUE
              ) - m
          )))
        }))
    }
    
    
    #Calculate log harmonic mean using log sum exp trick for numerical stability
    m<-max(likelihood_log)
    hme_log<- log(S) - log(exp(-m) * sum(exp(-likelihood_log + m)))
    est_simulation[k]<-hme_log
    
    time_simulation[k] <- (proc.time() - start)[3]
    
  }
  
  #Store results
  df_all <- rbind(
    df_all,
    data.frame(
      Estimate = est_simulation,
      Time     = time_simulation,   # â† ADD THIS LINE
      MC = factor(paste0("N = ", MC_sample),
                  levels = paste0("N = ", MC_values))
    )
  )
  
}


#Plotting evidence violin plots against MC sample size
ggplot(df_all, aes(x = MC, y = Estimate)) +
  geom_violin(aes(fill = MC),
              trim = FALSE,
              alpha = 0.7) +
  geom_boxplot(width = 0.12,
               fill = "white",
               outlier.shape = NA) +
  scale_color_manual(
    name = ""
  ) +
  labs(
    title = "Monte Carlo convergence of the Harmonic mean estimator",
    x = "N",
    y = "Log Evidence"
  ) +
  theme_minimal()+
  theme(
    legend.position = "right",
    legend.title = element_blank()
  )


#Mean runtime vs MC sample size
library(dplyr)

df_time <- df_all %>%
  group_by(MC) %>%
  summarise(
    mean_time = mean(Time),
    sd_time   = sd(Time),
    .groups = "drop"
  )

#ggplot(df_time, aes(x = MC, y = mean_time, group = 1)) +
#  geom_point(size = 3) +
#  geom_line() +
#  labs(
#    title = "Mean Runtime vs Monte Carlo Sample Size",
#    x = "Monte Carlo sample size",
#    y = "Mean runtime (seconds)"
#  ) +
#  theme_minimal()



#Plotting MC standard deviation against run time
df_error <- df_all %>%
  group_by(MC) %>%
  summarise(
    mc_sd = sd(Estimate),
  )
df_efficiency <- left_join(df_time, df_error, by = "MC")
ggplot(df_efficiency, aes(x = mean_time, y = mc_sd)) +
  geom_point(size = 3) +
  geom_line()+
  geom_text(aes(label = MC), vjust = 1.3, hjust=-0.1) +
  labs(
    title = "Monte Carlo efficiency - Harmonic mean estimator",
    x = "Mean runtime (seconds)",
    y = "Monte Carlo SD"
  ) +
#  coord_cartesian(xlim = c(NA, 80))+
  theme_minimal()
